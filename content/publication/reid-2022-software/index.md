---
title: 'Software engineering user study recruitment on prolific: An experience report'
authors:
- Brittany Reid
- Markus Wagner
- Marcelo d'Amorim
- Christoph Treude
date: '2022-01-01'
publishDate: '2025-07-23T05:21:02.014634Z'
publication_types:
- paper-conference
publication: '*International Workshop on Recruiting Participants for Empirical Software
  Engineering (RoPES)*'
abstract: 'Online participant recruitment platforms such as Prolific have been gaining
  popularity in research, as they enable researchers to easily access large pools
  of participants. However, participant quality can be an issue; participants may
  give incorrect information to gain access to more studies, adding unwanted noise
  to results. This paper details our experience recruiting participants from Prolific
  for a user study requiring programming skills in Node.js, with the aim of helping
  other researchers conduct similar studies. We explore a method of recruiting programmer
  participants using prescreening validation, attention checks and a series of programming
  knowledge questions. We received 680 responses, and determined that 55 met the criteria
  to be invited to our user study. We ultimately conducted user study sessions via
  video calls with 10 participants. We conclude this paper with a series of recommendations
  for researchers. '
tags:
- user studies
- participant recruitment
links:
- name: URL
  url: https://arxiv.org/abs/2201.05348
---
