@INPROCEEDINGS{abdullah2025using,
  author={Abdullah, Bayu Fedra and Nugroho, Yusuf Sulistyo and Reid, Brittany and Kula, Raula Gaikovina and Shimari, Kazumasa and Matsumoto, Kenichi},
  booktitle={2025 International Conference on Smart Computing, IoT and Machine Learning (SIML)}, 
  title={Using LLMs for Security Advisory Investigations: How Far are We?}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Large Language Models (LLMs) are increasingly used in software security, but their trustworthiness in generating accurate vulnerability advisories remains uncertain. This study investigates the ability of ChatGPT to (1) generate plausible security advisories from CVE-IDs, (2) differentiate real from fake CVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated dataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility and consistency of the model's outputs. The results show that ChatGPT generated plausible security advisories for 96 % of given input real CVE-IDs and $\mathbf{9 7 \%}$ of given input fake CVE-IDs, demonstrating a limitation in differentiating between real and fake IDs. Furthermore, when these generated advisories were reintroduced to ChatGPT to identify their original CVE-ID, the model produced a fake CVEID in 6% of cases from real advisories. These findings highlight both the strengths and limitations of ChatGPT in cybersecurity applications. While the model demonstrates potential for automating advisory generation, its inability to reliably authenticate CVE-IDs or maintain consistency upon re-evaluation underscores the risks associated with its deployment in critical security tasks. Our study emphasizes the importance of using LLMs with caution in cybersecurity workflows and suggests the need for further improvements in their design to improve reliability and applicability in security advisory generation.},
  keywords={Training;Accuracy;Large language models;Refining;Machine learning;Chatbots;Reliability engineering;Software;Prompt engineering;Computer security;advisory;chatgpt;cve id;security;vulnerability},
  doi={10.1109/SIML65326.2025.11080876},
  ISSN={},
  url={https://ieeexplore.ieee.org/document/11080876},
  month={June},
  type = {Conference Proceedings},
}

@article{chinthanet2021what,
   author = {Chinthanet, Bodin and Reid, Brittany and Treude, Christoph and Wagner, Markus and Kula, Raula Gaikovina and Ishio, Takashi and Matsumoto, Kenichi},
   title = {What makes a good Node. js package? Investigating Users, Contributors, and Runnability},
   journal = {arXiv preprint arXiv:2106.12239},
   abstract = {The Node.js Package Manager (i.e., npm) archive repository serves as a critical part of the JavaScript community and helps support one of the largest developer ecosystems in the world. However, as a developer, selecting an appropriate npm package to use or contribute to can be difficult. To understand what features users and contributors consider important when searching for a good npm package, we conduct a survey asking Node.js developers to evaluate the importance of 30 features derived from existing work, including GitHub activity, software usability, and properties of the repository and documentation. We identify that both user and contributor perspectives share similar views on which features they use to assess package quality. We then extract the 30 features from 104,364 npm packages and analyse the correlations between them, including three software features that measure package ``runnability"; ability to install, build, and execute a unit test. We identify which features are negatively correlated with runnability-related features and find that predicting the runnability of packages is viable. Our study lays the groundwork for future work on understanding how users and contributors select appropriate npm packages.},
   keywords = {library selection, documentation},
   url = {https://arxiv.org/abs/2106.12239},
   year = {2021},
   type = {Journal Article}
}

@inbook{jaisri2025preliminary,
  title={A Preliminary Study on Self-contained Libraries in the NPM Ecosystem},
  author={Jaisri, Pongchai and Reid, Brittany and Kula, Raula Gaikovina},
  editor="Lee, Roger",
  booktitle={Software Engineering and Management: Theory and Applications: Volume 17},
  abstract={The widespread of libraries within modern software ecosystems creates complex networks of dependencies. These dependencies are fragile to breakage, outdated, or redundancy, potentially leading to cascading issues in dependent libraries. One mitigation strategy involves reducing dependencies; libraries with zero dependencies become self-contained. This paper explores the characteristics of self-contained libraries within the NPM ecosystem. Analyzing a dataset of 2,763 NPM libraries, we found that 39.49{\%} are self-contained. Of these self-contained libraries, 40.42{\%} previously had dependencies that were later removed. This analysis revealed a significant trend of dependency reduction within the NPM ecosystem. The most frequently removed dependency was babel-runtime. Our investigation indicates that the primary reasons for dependency removal are concerns about the performance and the size of the dependency. Our findings illuminate the nature of self-contained libraries and their origins, offering valuable insights to guide software development practices.},
  volume={1193},
  year={2025},
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="53--65",
  publisher={Springer Nature},
  doi={10.1007/978-3-031-82610-8_4},
  isbn={978-3-031-82610-8},
  url={https://link.springer.com/chapter/10.1007/978-3-031-82610-8_4},
  type = {Book Chapter}
}

@inproceedings{kancharoendee2025categorizing,
   title={On Categorizing Open Source Software Security Vulnerability Reporting Mechanisms on GitHub},
   author={Kancharoendee, Sushawapak and Phichitphanphong, Thanat and Jongyingyos, Chanikarn and Reid, Brittany and Kula, Raula Gaikovina and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee},
   booktitle = {IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
   abstract = {Open-source projects are essential to software development, but publicly disclosing vulnerabilities without fixes increases the risk of exploitation. The Open Source Security Foundation (OpenSSF) addresses this issue by promoting robust security policies to enhance project security. Current research reveals that many projects perform poorly on OpenSSF criteria, indicating a need for stronger security practices and underscoring the value of SECURITY.md files for structured vulnerability reporting. This study aims to provide recommendations for improving security policies. By examining 679 open-source projects, we find that email is still the main source of reporting. Furthermore, we find that projects without SECURITY.md files tend to be less secure (lower OpenSSF scores). Our analysis also indicates that, although many maintainers encourage private reporting methods, some contributors continue to disclose vulnerabilities publicly, bypassing established protocols. The results from this preliminary study pave the way for understanding how developers react and communicate a potential security threat. Future challenges include understanding the impact and effectiveness of these mechanisms and what factors may influence how the security threat is addressed. },
   keywords = {open source, security policy, OpenSSF},
   url = {https://ieeexplore.ieee.org/document/10992562},
   keywords={Protocols;Security;Open source software;Software development management;open-source;security policy;OpenSSF},
   doi={10.1109/SANER64311.2025.00076},
   ISSN={2640-7574},
   month={March},
   year = {2025},
   volume={},
   number={},
   pages={751-756},
   type = {Conference Proceedings},
}

@INPROCEEDINGS{nugroho2025uncovering,
  author={Nugroho, Yusuf Sulistyo and Salam, Farah Danisha and Reid, Brittany and Kula, Raula Gaikovina and Shimari, Kazumasa and Matsumoto, Kenichi},
  booktitle={2025 International Conference on Smart Computing, IoT and Machine Learning (SIML)}, 
  title={Uncovering Intention Through LLM-Driven Code Snippet Description Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  url={https://ieeexplore.ieee.org/document/11081156},
  abstract={Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as “Example” (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library.},
  keywords={Codes;Large language models;Documentation;Machine learning;Libraries;code snippets;description;readme files;software documentation},
  doi={10.1109/SIML65326.2025.11081156},
  type = {Conference Proceedings},
}

@article{reid2023ncq,
   author = {Reid, Brittany and d'Amorim, Marcelo and Wagner, Markus and Treude, Christoph},
   title = {NCQ: code reuse support for Node. js developers},
   journal = {IEEE Transactions on Software Engineering},
   volume = {49},
   number = {5},
   pages = {3205-3225},
   abstract = {Code reuse is an important part of software development. The adoption of code reuse practices is especially common among Node.js developers. The Node.js package manager, NPM, indexes over 1 Million packages and developers often seek out packages to solve programming tasks. Due to the vast number of packages, selecting the right package is difficult and time consuming. With the goal of improving productivity of developers that heavily reuse code through third-party packages, we present Node Code Query (NCQ), a Read-Eval-Print-Loop environment that allows developers to 1) search for NPM packages using natural language queries, 2) search for code snippets related to those packages, 3) automatically correct errors in these code snippets, 4) quickly setup new environments for testing those snippets, and 5) transition between search and editing modes. In two user studies with a total of 20 participants, we find that participants begin programming faster and conclude tasks faster with NCQ than with baseline approaches, and that they like, among other features, the search for code snippets and packages. Our results suggest that NCQ makes Node.js developers more efficient in reusing code.},
   keywords = {code reuse, code search, library selection, documentation, search engines, search problems, task analysis, problem-solving},
   ISSN = {0098-5589},
   DOI = {10.1109/TSE.2023.3248113},
   url = {https://ieeexplore.ieee.org/abstract/document/10050780},
   year = {2023},
   type = {Journal Article}
}

@inproceedings{reid2020optimising,
   author = {Reid, Brittany and Treude, Christoph and Wagner, Markus},
   title = {Optimising the Fit of Stack Overflow Code Snippets into Existing Code},
   booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion (GECCO)},
   address = {New York, NY, USA},
   publisher = {Association for Computing Machinery},
   pages = {1945–1953},
   abstract = {Software developers often reuse code from online sources such as Stack Overflow within their projects. However, the process of searching for code snippets and integrating them within existing source code can be tedious. In order to improve efficiency and reduce time spent on code reuse, we present an automated code reuse tool for the Eclipse IDE (Integrated Developer Environment), NLP2TestableCode. NLP2TestableCode can not only search for Java code snippets using natural language tasks, but also evaluate code snippets based on a user's existing code, modify snippets to improve fit and correct errors, before presenting the user with the best snippet, all without leaving the editor. NLP2TestableCode also includes functionality to automatically generate customisable test cases and suggest argument and return types, in order to further evaluate code snippets. In evaluation, NLP2TestableCode was capable of finding compilable code snippets for 82.9% of tasks, and testable code snippets for 42.9%.},
   keywords = {optimisation, stack overflow, crowd-generated code snippets},
   DOI = {10.1145/3377929.3398087},
   url = {https://doi.org/10.1145/3377929.3398087},
   type = {Conference Proceedings},
   year = {2020}
}

@inproceedings{reid2023using,
   author = {Reid, Brittany and Treude, Christoph and Wagner, Markus},
   title = {Using the TypeScript compiler to fix erroneous Node. js snippets},
   booktitle = {2023 IEEE 23rd International Working Conference on Source Code Analysis and Manipulation (SCAM)},
   publisher = {Bogotá, Colombia},
   pages = {220-230},
   abstract = {Most online code snippets do not run. This means that developers looking to reuse code from online sources must manually find and fix errors. We present an approach for automatically evaluating and correcting errors in Node.js code snippets: Node Code Correction (NCC). NCC leverages the ability of the TypeScript compiler to generate errors and inform code corrections through the combination of TypeScript’s builtin codefixes, our own targeted fixes, and deletion of erroneous lines. Compared to existing approaches using linters, our findings suggest that NCC is capable of detecting a larger number of errors per snippet and more error types, and it is more efficient at fixing snippets. We find that 73.7% of the code snippets in NPM documentation have errors; with the use of NCC’s corrections, this number was reduced to 25.1%. Our evaluation confirms that the use of the TypeScript compiler to inform code corrections is a promising strategy to aid in the reuse of code snippets from online sources.},
   keywords = {node.js, static analysis, error correction, documentation},
   DOI = {10.1109/SCAM59687.2023.00031},
   url = {https://ieeexplore.ieee.org/abstract/document/10356712},
   type = {Conference Proceedings},
   year = {2023}
}

@inproceedings{reid2022software,
   author = {Reid, Brittany and Wagner, Markus and d'Amorim, Marcelo and Treude, Christoph},
   title = {Software engineering user study recruitment on prolific: An experience report},
   booktitle = {International Workshop on Recruiting Participants for Empirical Software Engineering (RoPES)},
   abstract = {Online participant recruitment platforms such as Prolific have been gaining popularity in research, as they enable researchers to easily access large pools of participants. However, participant quality can be an issue; participants may give incorrect information to gain access to more studies, adding unwanted noise to results. This paper details our experience recruiting participants from Prolific for a user study requiring programming skills in Node.js, with the aim of helping other researchers conduct similar studies. We explore a method of recruiting programmer participants using prescreening validation, attention checks and a series of programming knowledge questions. We received 680 responses, and determined that 55 met the criteria to be invited to our user study. We ultimately conducted user study sessions via video calls with 10 participants. We conclude this paper with a series of recommendations for researchers. },
   keywords = {user studies, participant recruitment},
   url = {https://arxiv.org/abs/2201.05348},
   type = {Conference Proceedings},
   year = {2022}
}

@phdthesis{reid2023improving,
   author = {Reid, Brittany},
   title = {Improving Developer Efficiency through Code Reuse},
   university = {The University of Adelaide},
   abstract = {Code reuse is an integral part of modern software development, where most software is built using existing software artefacts. Ranging from the copy-pasting of code fragments to the use of third-party libraries, developers frequently turn to the internet to find already-made solutions to difficult programming tasks and save development time. However, the large amount of libraries and code online can make finding the best solution difficult, and reuse is not necessarily straightforward. Most online code snippets do not run, meaning developers need to spend time correcting errors, and when example code snippets are meant to demonstrate API usage, this can present a barrier to using new libraries. This work studies ways to aid developers in the code reuse process, in order to improve their efficiency. We look at ways to more easily connect developers to the wealth of libraries and usage examples online from within their programming environment with our tool for Node.js, Node Code Query (NCQ). We then evaluate how well developers perform compared to the conventional code reuse process and found that developers using our tool solve tasks faster and have to try fewer libraries. Additionally, we study what problems online Node.js code snippets have and how to best correct them automatically, to save developers time in this step of the reuse process. We find that through the combination of the TypeScript compiler’s error detection and codefixes, and our line deletion and custom fixes, we can increase the percentage error-free snippets in our dataset from 26.3% to 74.94%. Finally, we compare the emerging AI code snippet generation and pair programmer technologies to current online code snippet reuse practices, particularly looking at how snippets generated by GitHub’s Copilot extension and those retrieved from Stack Overflow using Google might differ. We find that for the same set of queries, Copilot returned more snippets, with fewer errors and that were more relevant. Ultimately, this work provides further evidence of how automating the code reuse process can improve developer efficiency, and proposes a series of solutions to that end. Additionally, we provide a comparison between existing and emerging reuse processes. As the state of code reuse changes, helping developers understand the strengths of weaknesses of these approaches will become increasingly important.},
   keywords = {software engineering, code reuse, code correction, code generation, code search, static analysis, node.js, javascript, java, python},
   url = {https://hdl.handle.net/2440/139921},
   year = {2023},
   type = {Thesis}
}